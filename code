import cv2
import time
import numpy as np
from jetbot import Camera, Robot, bgr8_to_jpeg
from IPython.display import display
import ipywidgets.widgets as widgets
import torch
import math
import traitlets
from collections import deque


# Initialize the camera and robot
camera = Camera.instance(width=224, height=224)
robot = Robot()

# Display widget for the processed image
image_widget = widgets.Image(format='jpeg', width=300, height=300)
display(image_widget)

detection_queue = deque(maxlen=10)
robot_state = "line follow"  # Initial state

# Define preprocessing functions for edge detection
def preprocess_image_remove_noise(frame):
    """Sharpen the image to enhance edges."""
    kernel_size = (5, 5)
    std_dev = 1.0
    blurred = cv2.GaussianBlur(frame, kernel_size, std_dev)
    high_freq = cv2.subtract(frame, blurred)
    weight = 1.5
    sharpened = cv2.addWeighted(frame, 1, high_freq, weight, 0)
    return sharpened

def color_transformation_HSV(sharpened):
    """Convert the image to HSV and isolate the black areas."""
    hsv_image = cv2.cvtColor(sharpened, cv2.COLOR_BGR2HSV)
    lower_black = np.array([0, 0, 0])
    upper_black = np.array([180, 255, 50])
    mask = cv2.inRange(hsv_image, lower_black, upper_black)
    return mask


def isolate_region_of_interest(mask):
    """Limit the ROI to the bottom-middle portion of the frame."""
    height, width = mask.shape[:2]
    #roi = mask[0 : height, 0 : width ]
    #roi = mask[int(height * 0.7) : height, width // 10 : width * 9 // 10]
    roi = mask[height // 2 : height, width // 10 : width * 9 // 10]

#     roi = mask[height // 2 : height, width // 10 : width * 9 // 10]
    return roi


def isolate_region_of_interest_even_more(mask):
    """Limit the ROI to the bottom-bottom-middle portion of the frame."""
    height, width = mask.shape[:2]
    roi = mask[height // 4 : height, width // 10 : width * 9 // 10]
    return roi


def canny_edge_detection(roi):
    """Detect edges in the ROI using Canny edge detection."""
    lowthreshold = 100
    highthreshold = 200
    edges = cv2.Canny(roi, lowthreshold, highthreshold)
    return edges

       
def green_mask(image, edges):
   
    """Detect green and its location, provide adequete instructions"""
   
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
   
    #define green HSV range, will need to alter
    lower_green = np.array([80, 97, 73])  # Adjust based on lighting
    upper_green = np.array([110, 255, 255])
   
    # Create a mask for green areas
    mask = cv2.inRange(hsv, lower_green, upper_green)
   
    left_detected = False
    right_detected = False
    left_detected_r = False
    right_detected_r = False
    round_about = False
   
    # Get image dimensions
    height, width = mask.shape[:2]
    mid_x = width // 2
   
     # Find contours in the mask
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
   
    for contour in contours:
        if cv2.contourArea(contour) < 100:  # Ignore small noise
            continue
           
        print("green detected")
           
        # Get the bounding box and centroid
        x, y, w, h = cv2.boundingRect(contour)
        centroid_x = x + w // 2
        centroid_y = y + h //2
       
        moments = cv2.moments(edges)
        if moments["m00"] > 0:  # Check for edge presence
            #k_p = 0.30
            x_mean = int(moments["m10"] / moments["m00"])  # x-coordinate of the centroid
            y_mean = int(moments["m01"] / moments["m00"])  # y-coordinate of the centroid

                   
           
            print("centroid x green:", centroid_x, "mean of black x ", x_mean)
            print("centroid y green:", centroid_y, "mean of black y ", y_mean)

           
            #to detect turn in round about
            if (abs(centroid_x - x_mean) < 14) and (abs(centroid_x - x_mean) > 5): #3
                round_about = True
                print("roundddddddd")
                if centroid_x < x_mean:
                    left_detected_r = True
               
                else:  # Right side
                    right_detected_r = True
                continue
           
            #to avoid green after black --> luck
            elif (abs(centroid_y - y_mean) < 40): #44
                print("avoid")
                None
           
            elif (centroid_x < x_mean) and (abs(centroid_x - x_mean) > 18): #20
                left_detected = True
               
            elif (centroid_x > x_mean) and (abs(centroid_x - x_mean) > 18): #20
                right_detected = True
               
            else:
                print ("false green")
                None
               
    # Decision logic
    if left_detected and right_detected:
        return "u-turn" # Green on both sides: Dead end
    elif left_detected:
        return "left"  # Green on the left
    elif right_detected:
        return "right"  # Green on the right
    elif left_detected_r:
        return "roundl"  # Green on the left
    elif right_detected_r:
        return "roundr"  # Green on the right
   
    else:
        return "straight"  # No green marker: Continue straight
   
def detect_red(image):
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
   
    lower_red_1 = np.array([0, 120, 70])
    upper_red_1 = np.array([10, 255, 255])
    lower_red_2 = np.array([170, 120, 70])
    upper_red_2 = np.array([179, 255, 255])

    mask1 = cv2.inRange(hsv, lower_red_1, upper_red_1)
    mask2 = cv2.inRange(hsv, lower_red_2, upper_red_2)
    mask = cv2.bitwise_or(mask1, mask2)

    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
   
    # Iterate through the contours to check for significant red regions
    for contour in contours:
        area = cv2.contourArea(contour)
        #print("Contour Area:", area)
        # Ignore small noise by checking the area of the contour
        if cv2.contourArea(contour) < 5000: #5000
            continue
       
        # If a significant red contour is found, return True
        #print("red")
        return True

    # If no significant red regions are found, return False
    #print("no read")
    robot_state = "line follow"  
    return False


def detect_block(image):
    # Convert to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
   
    # Blur to reduce noise
    blur = cv2.GaussianBlur(gray, (5, 5), 0)
   
    # Threshold to isolate dark objects on light background
    _, thresh = cv2.threshold(blur, 60, 255, cv2.THRESH_BINARY_INV)
   
    # Morphological open to remove thin lines and small noise
    kernel = np.ones((3, 3), np.uint8)
    cleaned = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)
   
    # Find contours of remaining blobs
    contours, _ = cv2.findContours(cleaned, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
   
    # Check if any contour is large enough to be considered an obstacle
    for contour in contours:
        area = cv2.contourArea(contour)
        # Filter based on a reasonable area threshold
        if area > 6500:
            print("Obstacle detected!")
            return True
   
    # No large obstacle detected
    return False

angle = 0.0
angle_last = 0.0

speed = 0.12 #0.12
k_p = 0.25 #0.3
k_d = 1.2
robot_state = "line follow"
#zig_complete = False
u_turn_complete = False  # Flag to track U-turn completion
right_complete = False
left_complete = False
cooldown_start_time = 0  # Time when cooldown starts
cooldown_duration = 4  # Cooldown duration in seconds
zig_do = False
zig_complete = False
roundright_complete = False
roundleft_complete = False

zig_do = False
zig_complete = False

block_detected_count = 0
block_absent_count = 0
block_threshold = 5   # number of consecutive frames needed to confirm block
unblock_threshold = 5 # number of consecutive frames needed to confirm unblock
is_blocked = False    # current state


no_line_timeout = 3.0  # seconds before performing the search maneuver
last_line_seen_time = time.time()
line_search_performed = False

# Indicates the current search mode: None, 'left', or 'right'
line_search_mode = None
searched_left_already = False


# ...

camera.observe(execute, names='value')
#traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)



