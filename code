import cv2
import time
import numpy as np
from jetbot import Camera, Robot, bgr8_to_jpeg
from IPython.display import display
import ipywidgets.widgets as widgets
import torch
import math
import traitlets
from collections import deque


# Initialize the camera and robot
camera = Camera.instance(width=224, height=224)
robot = Robot()

# Display widget for the processed image
image_widget = widgets.Image(format='jpeg', width=300, height=300)
display(image_widget)

detection_queue = deque(maxlen=10)
robot_state = "line follow"  # Initial state

# Define preprocessing functions for edge detection
def preprocess_image_remove_noise(frame):
    """Sharpen the image to enhance edges."""
    kernel_size = (5, 5)
    std_dev = 1.0
    blurred = cv2.GaussianBlur(frame, kernel_size, std_dev)
    high_freq = cv2.subtract(frame, blurred)
    weight = 1.5
    sharpened = cv2.addWeighted(frame, 1, high_freq, weight, 0)
    return sharpened

def color_transformation_HSV(sharpened):
    """Convert the image to HSV and isolate the black areas."""
    hsv_image = cv2.cvtColor(sharpened, cv2.COLOR_BGR2HSV)
    lower_black = np.array([0, 0, 0])
    upper_black = np.array([180, 255, 50])
    mask = cv2.inRange(hsv_image, lower_black, upper_black)
    return mask


def isolate_region_of_interest(mask):
    """Limit the ROI to the bottom-middle portion of the frame."""
    height, width = mask.shape[:2]
    #roi = mask[0 : height, 0 : width ]
    #roi = mask[int(height * 0.7) : height, width // 10 : width * 9 // 10]
    roi = mask[height // 2 : height, width // 10 : width * 9 // 10]

#     roi = mask[height // 2 : height, width // 10 : width * 9 // 10]
    return roi


def isolate_region_of_interest_even_more(mask):
    """Limit the ROI to the bottom-bottom-middle portion of the frame."""
    height, width = mask.shape[:2]
    roi = mask[height // 4 : height, width // 10 : width * 9 // 10]
    return roi


def canny_edge_detection(roi):
    """Detect edges in the ROI using Canny edge detection."""
    lowthreshold = 100
    highthreshold = 200
    edges = cv2.Canny(roi, lowthreshold, highthreshold)
    return edges

       
def green_mask(image, edges):
   
    """Detect green and its location, provide adequete instructions"""
   
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
   
    #define green HSV range, will need to alter
    lower_green = np.array([80, 97, 73])  # Adjust based on lighting
    upper_green = np.array([110, 255, 255])
   
    # Create a mask for green areas
    mask = cv2.inRange(hsv, lower_green, upper_green)
   
    left_detected = False
    right_detected = False
    left_detected_r = False
    right_detected_r = False
    round_about = False
   
    # Get image dimensions
    height, width = mask.shape[:2]
    mid_x = width // 2
   
     # Find contours in the mask
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
   
    for contour in contours:
        if cv2.contourArea(contour) < 100:  # Ignore small noise
            continue
           
        print("green detected")
           
        # Get the bounding box and centroid
        x, y, w, h = cv2.boundingRect(contour)
        centroid_x = x + w // 2
        centroid_y = y + h //2
       
        moments = cv2.moments(edges)
        if moments["m00"] > 0:  # Check for edge presence
            #k_p = 0.30
            x_mean = int(moments["m10"] / moments["m00"])  # x-coordinate of the centroid
            y_mean = int(moments["m01"] / moments["m00"])  # y-coordinate of the centroid

                   
           
            print("centroid x green:", centroid_x, "mean of black x ", x_mean)
            print("centroid y green:", centroid_y, "mean of black y ", y_mean)

           
            #to detect turn in round about
            if (abs(centroid_x - x_mean) < 14) and (abs(centroid_x - x_mean) > 5): #3
                round_about = True
                print("roundddddddd")
                if centroid_x < x_mean:
                    left_detected_r = True
               
                else:  # Right side
                    right_detected_r = True
                continue
           
            #to avoid green after black --> luck
            elif (abs(centroid_y - y_mean) < 40): #44
                print("avoid")
                None
           
            elif (centroid_x < x_mean) and (abs(centroid_x - x_mean) > 18): #20
                left_detected = True
               
            elif (centroid_x > x_mean) and (abs(centroid_x - x_mean) > 18): #20
                right_detected = True
               
            else:
                print ("false green")
                None
               
    # Decision logic
    if left_detected and right_detected:
        return "u-turn" # Green on both sides: Dead end
    elif left_detected:
        return "left"  # Green on the left
    elif right_detected:
        return "right"  # Green on the right
    elif left_detected_r:
        return "roundl"  # Green on the left
    elif right_detected_r:
        return "roundr"  # Green on the right
   
    else:
        return "straight"  # No green marker: Continue straight
   
def detect_red(image):
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
   
    lower_red_1 = np.array([0, 120, 70])
    upper_red_1 = np.array([10, 255, 255])
    lower_red_2 = np.array([170, 120, 70])
    upper_red_2 = np.array([179, 255, 255])

    mask1 = cv2.inRange(hsv, lower_red_1, upper_red_1)
    mask2 = cv2.inRange(hsv, lower_red_2, upper_red_2)
    mask = cv2.bitwise_or(mask1, mask2)

    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
   
    # Iterate through the contours to check for significant red regions
    for contour in contours:
        area = cv2.contourArea(contour)
        #print("Contour Area:", area)
        # Ignore small noise by checking the area of the contour
        if cv2.contourArea(contour) < 5000: #5000
            continue
       
        # If a significant red contour is found, return True
        #print("red")
        return True

    # If no significant red regions are found, return False
    #print("no read")
    robot_state = "line follow"  
    return False


def detect_block(image):
    # Convert to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
   
    # Blur to reduce noise
    blur = cv2.GaussianBlur(gray, (5, 5), 0)
   
    # Threshold to isolate dark objects on light background
    _, thresh = cv2.threshold(blur, 60, 255, cv2.THRESH_BINARY_INV)
   
    # Morphological open to remove thin lines and small noise
    kernel = np.ones((3, 3), np.uint8)
    cleaned = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)
   
    # Find contours of remaining blobs
    contours, _ = cv2.findContours(cleaned, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
   
    # Check if any contour is large enough to be considered an obstacle
    for contour in contours:
        area = cv2.contourArea(contour)
        # Filter based on a reasonable area threshold
        if area > 6500:
            print("Obstacle detected!")
            return True
   
    # No large obstacle detected
    return False

angle = 0.0
angle_last = 0.0

speed = 0.12 #0.12
k_p = 0.25 #0.3
k_d = 1.2
robot_state = "line follow"
#zig_complete = False
u_turn_complete = False  # Flag to track U-turn completion
right_complete = False
left_complete = False
cooldown_start_time = 0  # Time when cooldown starts
cooldown_duration = 4  # Cooldown duration in seconds
zig_do = False
zig_complete = False
roundright_complete = False
roundleft_complete = False

zig_do = False
zig_complete = False

block_detected_count = 0
block_absent_count = 0
block_threshold = 5   # number of consecutive frames needed to confirm block
unblock_threshold = 5 # number of consecutive frames needed to confirm unblock
is_blocked = False    # current state


no_line_timeout = 3.0  # seconds before performing the search maneuver
last_line_seen_time = time.time()
line_search_performed = False

# Indicates the current search mode: None, 'left', or 'right'
line_search_mode = None
searched_left_already = False



def execute(change):
   
    global zig_do, zig_complete, angle, angle_last, speed, k_p, k_d, robot_state, u_turn_complete, cooldown_start_time, roundleft_complete, roundright_complete, right_complete, left_complete, zig_complete
    global is_blocked, block_detected_count, block_absent_count
    global last_line_seen_time, line_search_performed
    """Process each frame from the camera."""
    frame = change['new']
   
    #print("Kp:", k_p )
    # Preprocess image
    sharpened = preprocess_image_remove_noise(frame)
    mask = color_transformation_HSV(sharpened)
    roi = isolate_region_of_interest(mask) #_even_more
    green_ra_roi = isolate_region_of_interest(mask)
    edges = canny_edge_detection(roi)
    green_edge = canny_edge_detection(green_ra_roi)

   
    #intersection = detect_intersection_shape(roi)
   
    green_roi = isolate_region_of_interest(frame) #same as below, just used for naming
   
    red_roi = isolate_region_of_interest(frame)
   
    red_detected = detect_red(red_roi)
   
    blockedy = detect_block(frame)
   
    if blockedy:
        block_detected_count += 1
        block_absent_count = 0
    else:
        block_absent_count += 1
        block_detected_count = 0
   
    # State transitions for block
    if robot_state == "line follow":
        # If blocking is confirmed, transition to blocked state
        if block_detected_count >= block_threshold:
            robot_state = "blocked"
            print("Transitioning to 'blocked' state.")
            robot.stop()

    elif robot_state == "blocked":
        # If no block for some time, go back to line follow
        if block_absent_count >= unblock_threshold:
            robot_state = "line follow"
            print("Transitioning back to 'line follow' state.")
   
    direction = green_mask(green_roi, green_edge)

    zig_time = time.time()
   
    moments = cv2.moments(edges)
    if moments["m00"] > 0:  # Check for edge presence
       
        x_mean = int(moments["m10"] / moments["m00"])  # x-coordinate of the centroid
        y_mean = int(moments["m01"] / moments["m00"])  # y-coordinate of the centroid
       
        # If we just found a line after being lost, reset the search state
        last_line_seen_time = time.time()
        line_search_performed = False
       
        if u_turn_complete == True:
            k_p = 0.25
            speed = 0.12
        else:
            k_p = 0.25
            speed = 0.12
            #speed = 0.12
    else:
        #speed = 0.15 #DELETE
        k_p = 0.2
        #speed = 0.10
        #print("default entererd")
        x_mean, y_mean = green_ra_roi.shape[1] // 2, green_ra_roi.shape[0] // 2  # Default to center
        current_time = time.time()


    # Normalize x_mean to [-1, 1] for steering control
    normalized_x = (x_mean - roi.shape[1] / 2) / (roi.shape[1] / 2)
    steering_angle = normalized_x * k_p#steering_gain_slider.value
    # Calculate the derivative component
    angle_derivative = (steering_angle - angle_last) * k_d #steering_dgain_slider.value
    angle_last = steering_angle  # Update for the next frame

    # Combine proportional and derivative components
    smoothed_steering = steering_angle + angle_derivative
   
    #robot_state = "line follow"
    #start_time = 0

    current_time = time.time()

    # Initialize a timer variable for holding the green ROI
    green_hold_start_time = None
    green_hold_duration = 10.0  # Duration to hold green ROI (in seconds)

    # State-based robot behavior
    if robot_state == "line follow":
        robot.left_motor.value = max(speed + smoothed_steering, 0.0)
        robot.right_motor.value = max(speed - smoothed_steering, 0.0)
       
        # Line-following motor control
        if direction == "u-turn" and not u_turn_complete:
            #k_p = 0.2
            print("U-turn detected! Transitioning to uturn state.")
            u_turn_complete = True
            robot.stop()
            time.sleep(1.0)
            robot.set_motors(-0.2, 0.2)
            time.sleep(0.8)
#             robot.stop()
#             time.sleep(0.2)
            cooldown_start_time = current_time
            direction = "straight"
           
        elif direction == "right" and not right_complete:
            zig_do = True
            #zig_do = True
            #speed = 0.10
            print("right")
            right_complete = True
            robot.stop()
            #time.sleep(0.1)
            robot.forward(speed)
            time.sleep(1.5)
            robot.set_motors(0.2, -0.2)
            time.sleep(0.45)
#             robot.stop()
#             time.sleep(0.2)
            cooldown_start_time = current_time
            direction = "straight"

        elif direction == "left" and not left_complete:
            print("left")
            left_complete = True
            robot.stop()
            #time.sleep(1.0)
            robot.forward(speed)
            time.sleep(1.5)
            robot.set_motors(-0.2, 0.2)
            time.sleep(0.45)
#             robot.stop()
#             time.sleep(0.5)
            cooldown_start_time = current_time
            direction = "straight"
       
        elif direction == "roundr" and not roundright_complete:
            roundright_complete = True
            robot.stop()
            time.sleep(1.0)
            robot.forward(speed)
            time.sleep(0.6)
            robot.set_motors(0.2, -0.2)
            time.sleep(0.3) #0.25
            robot.stop()
            time.sleep(0.5)
            cooldown_start_time = current_time
            direction = "straight"
            print("round and right")
       
        elif direction == "roundl" and not roundleft_complete:
            roundleft_complete = True
            robot.stop()
            time.sleep(1.0)
            robot.forward(speed)
            time.sleep(0.6)
            robot.set_motors(-0.2, 0.2)
            time.sleep(0.4) #0.2
            robot.stop()
            time.sleep(0.5)
            cooldown_start_time = current_time
            direction = "straight"
            print("round and left")
           
        elif red_detected == True:
            print("red")
            robot.stop()
            #time.sleep(10)
            robot_state = "red"
           
    elif robot_state == "blocked":
        # Stay stopped while blocked
        robot.stop()

    elif robot_state == "red":
        robot.stop()
        time.sleep(0.5)
        print("red")
       
        robot_state = "line follow"
       
    if right_complete and (current_time - cooldown_start_time > cooldown_duration):
        print("Cooldown ended. Ready to detect right again.")
        right_complete = False
        direction = "straight"

    if left_complete and (current_time - cooldown_start_time > cooldown_duration):
        print("Cooldown ended. Ready to detect left again.")
        left_complete = False
        direction = "straight"
       
    if roundright_complete and (current_time - cooldown_start_time > cooldown_duration):
        print("Cooldown ended. Ready to detect round right again.")
        roundright_complete = False
        direction = "straight"
       
    if roundleft_complete and (current_time - cooldown_start_time > cooldown_duration):
        print("Cooldown ended. Ready to detect round right again.")
        roundleft_complete = False
        direction = "straight"

    # Replace gree_roi with green_roi wherever it appears
    roi_color = cv2.cvtColor(roi, cv2.COLOR_GRAY2BGR)
    green_roi_jpeg = bgr8_to_jpeg(roi_color)
    image_widget.value = green_roi_jpeg
   

camera.observe(execute, names='value')
#traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)



